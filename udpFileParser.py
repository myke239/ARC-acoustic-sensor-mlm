import pyshark
import struct
import binascii
import sys
import os.path
from os import path
import matplotlib.pyplot as plt
import optparse
import time
import numpy as np
import pandas as pd
import joblib
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns




from numpy.random import seed
import tensorflow as tf 
from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector
from keras.models import Model, load_model
from keras import regularizers


LOGGING_SILENT = 0
LOGGING_ERROR = 1
LOGGING_WARN = 2
LOGGING_INFO = 3
LOGGING_DEBUG = 4

VERBOSE = 3

#Based on CEL Fiber Strike UDP packet format
PACKET_ID_OFFSET = 16
#Based on CEL Fiber Strike UDP packet format
PACKET_ID_SIZE = 8
#Based on CEL Fiber Strike UDP packet format
SENSOR_DATA_OFFSET = 32
#Based on CEL Fiber Strike UDP packet format
#16 channels, 2 bytes each, 64 characters
SENSOR_DATA_ROW_SIZE = 16 * 4 
#2 bytes 4 characters
CH_DATA_SIZE = 4 
#Based on CEL Fiber Strike UDP packet format
DATA_ROWS_PER_PACKET = 16
#Number of channels to read data from. 
NUM_ACTIVE_CHANNELS = 4

#Number of tics when displaying loading animation
MAX_TICS = 100 





def set_seed():
    #set random seed
    seed(10)
    tf.random.set_seed(10)

def twos_comp(val, bits):
    """compute the 2's complement of int value val"""
    if (val & (1 << (bits - 1))) != 0: # if sign bit is set e.g., 8bit: 128-255
        val = val - (1 << bits)        # compute negative value
    return val

def autoencoder_model(X):
    inputs = Input(shape=(X.shape[1], X.shape[2]))
    L1 = LSTM(16, activation='relu', return_sequences=True, kernel_regularizer=regularizers.l2(0.00))(inputs)
    L2 = LSTM(4, activation='relu', return_sequences=False)(L1)
    L3 = RepeatVector(X.shape[1])(L2)
    L4 = LSTM(4, activation='relu', return_sequences=True)(L3)
    L5 = LSTM(16, activation='relu', return_sequences=True)(L4)
    output=TimeDistributed(Dense(X.shape[2]))(L5)
    model = Model(inputs=inputs, outputs=output)
    return model


def optargs():
    parser = optparse.OptionParser("usage: %prog [options]")

    parser.add_option("-c", "--command", dest="command", default="none", type="string", \
        help="Specify the action to be done by the script.\n\
            parse - parse a raw file and out put a formatted structure for later use.\n\
            train - train the specified model with the -m option with the specified data.\n\
            predict - get a prediction from a previosuly trained model specified with the -m option")
    
    parser.add_option("-t", "--file-type", dest="file_type", default="none", type="string", \
        help="Specify type of file being specified with the -i option.")

    parser.add_option("-T", "--threshold", dest="threshold", default=.1, type="float", \
        help="Threshold value for plotting predictions loss mae threshold.")

    parser.add_option("-o", "--output-dir", dest="output_dir", default="./", type="string", \
        help="Specify path to a directory to output generated files; if not path specified, files will be output to local path.")

    parser.add_option("-m", "--model", dest="model", default="none", type="string", \
        help="Specify path to a machine learning model (MLM) to be used for training or predictions.")

    parser.add_option("-i", "--input", dest="input_file", default="none", type="string", \
        help="Specify path to additional file to read input in from. Must specify file type with -t option.")

    parser.add_option("-d", "--input-directory", dest="input_dir", default="none", type="string", \
        help="Specify where core files generated by parse command are located to import for use with mlm.\n\
            \
            Finish path with '/' for directory or leave without for a common prefix.")

    parser.add_option("-f", "--display-filter", dest="display_filter", default="udp", type="string", \
        help="Specify a WireShark display filter to sort packets by. Default filter is 'udp'.\
            Example: %prog -f 'ip.src==192.168.0.0/16' myFile.pcap \n\
*           Note:It is recommended to filter the capture file in WireShark first before running this script.")

    parser.add_option("-n", "--packets", dest="num_packets", default=0, type="int", \
        help="Specify a number of packets to read from file. If no specified, the entire file will be read.")

    parser.add_option("-I", "--start-index", dest="start_index", default=0, type="int", \
        help="Specify the index value of the first packet to read from in the capture file. Default = 0")

    parser.add_option("-P", "--plot", action="store_true", dest="plot", default=False, \
        help="Displays graphical plot of sensor data extracted from packet capture file.")

    parser.add_option("-L", "--log-level", dest="log_level", default=1, type="int", \
        help="Set verbose mode logging level.\n\
        0 : Silent - Nothig will be printed to stdout.\n\
        1 : Error - Only failures and fatal errors will be printed to stdout.\n\
        2 : Warn - Warnings and recommendations will be printed to stdout.\n\
        3 : Info - Runtime information, statistics, and progress statuses will be printed to stdout.\n\
        4 : Debug - Variables and raw data will be printed to stdout.")

    
    return parser









def parse_pcap(filepath, display_filters, active_channels, start_index, num_packets):

    if VERBOSE >= LOGGING_INFO:
        print("Reading packet capture file {} \n\twith options : display_filters=\'{}\'".format( filepath, display_filters))

    cap = pyshark.FileCapture(filepath, display_filter=display_filters)

    if num_packets > 0 :
        NUM_PACKETS = num_packets
        loading_increment = NUM_PACKETS / MAX_TICS
        if VERBOSE >= LOGGING_DEBUG :
            print("Loading Increment:", loading_increment)
    else:
        if VERBOSE >= LOGGING_WARN:
            print("WARNING:\n\tNo max packet count set! Packets will be analyzed till EOF reached. Large files may take a long time to process.\n")
        if VERBOSE != LOGGING_SILENT:
            print("\nPress Ctrl+c to stop reading new packets and continue...\n")

    if VERBOSE >= LOGGING_INFO:
        print("Starting capture file processing starting with packet index {}".format(start_index))





    packets = {}
    packet_delta_t = np.array([]) 
    failed_packets = []
    packet = cap[start_index]
    packet_index = start_index
    packet_count = 0
    start_time = time.time()

    while packet_count < NUM_PACKETS :
    #Extract UDP Packet Contents
        
        try:
            packet = cap.next()

            temp = packet.data.data[PACKET_ID_OFFSET: PACKET_ID_OFFSET + PACKET_ID_SIZE]
            
            packet_id = temp[6:] + temp[4:6] + temp[2:4] + temp[:2]
            packet_id = int(packet_id, 16)

            rows = np.empty([16,4])
            for i in range(DATA_ROWS_PER_PACKET):            
                #row_id = packet_id + format(i, 'x')
                #data[0].append(int(row_id, 16))
                for j in range(active_channels):
                    offset = SENSOR_DATA_OFFSET + (SENSOR_DATA_ROW_SIZE * i) + (CH_DATA_SIZE * j)
                    value = packet.data.data[offset:  offset + CH_DATA_SIZE ]
                    temp = value[:2]
                    value = value[2:]
                    value = value + temp
                    value = (twos_comp(int(value, 16), 16) / 1000)
                    #data[j+1].append(value)
                    rows[i][j] = value

            packets[packet_id] = rows
            packet_delta_t = np.append(packet_delta_t, float(packet.udp.time_delta))

            packet_count += 1
            packet_index += 1
            if options.num_packets == 0 :
                NUM_PACKETS = packet_count + 1
                loading = "Packets Processed: |{:>12,}|".format(packet_count)
                print(loading, end='\r')
            else:
                bar= ("#" * int(packet_count / loading_increment))
                percent = (packet_count/NUM_PACKETS)
                print("{:>127}".format("|"), end='\r')
                loading = "Reading Packets|{:00.0%}| {}".format(percent, bar)
                print(loading, end='\r')
            
        except EOFError:
            if VERBOSE >= LOGGING_ERROR:
                print("EOF Error: End of packet capture reached.")
            break
        except StopIteration:
            if VERBOSE != LOGGING_SILENT:
                print("Packet capture file reading stopped.")
            break
        except AttributeError:
            if VERBOSE >= LOGGING_WARN:
                print("Reading Packet (index) {} failed with AttributeError.".format( packet_index) )
            failed_packets.append(packet)

        except:
            if VERBOSE >= LOGGING_ERROR:
                print("Reading Packet (index) {} failed with an unexpected error.".format( packet_index ))
                print(sys.exc_info()[0])
                #print(packet)
            failed_packets.append(packet)
            
    print()   
    parse_time = time.time() - start_time
    
    if VERBOSE >= LOGGING_DEBUG :
        print("Closing packet capture file...")
    cap.close()

    if VERBOSE >= LOGGING_INFO:
        print ("{:,} Packets processed in {:0.3} seconds.".format(packet_count, parse_time))

    stats_dt = {\
        "Mean" : np.mean(packet_delta_t),\
        "Median" : np.median(packet_delta_t),\
        "Standard Deviation" : np.std(packet_delta_t),\
        "Variance" : np.var(packet_delta_t)}

    if VERBOSE >= LOGGING_INFO:
        for val in stats_dt:
            print("\t{:>30} {:<6.00000} seconds".format(" ", stats_dt[val]), end='\r')
            print("\t{}:".format(val))

    return stats_dt, packets


def parse_packets(packets, packet_count, avg_x_packets, active_channels):
    NUM_PACKETS = packet_count
    packet_count = 0
    loading_increment = NUM_PACKETS / MAX_TICS

    capture_data = pd.DataFrame()
    df = pd.DataFrame()
    for p in packets:
        packet_count += 1

        d = pd.DataFrame(packets[p])
        df = df.append(d)

        if packet_count % avg_x_packets == 0 :
            p_abs_mean = np.array(df.abs().mean())
            p_abs_mean = pd.DataFrame(p_abs_mean.reshape(1,4))
            p_abs_mean.index = [p]
            capture_data = capture_data.append(p_abs_mean)
            df = pd.DataFrame()

        bar= ("#" * int(packet_count / loading_increment))
        percent = (packet_count/NUM_PACKETS)
        print("{:>127}".format("|"), end='\r')
        loading = "Parsing Packets|{:00.0%}| {}".format(percent, bar)
        print(loading, end='\r')
            

    columns = []
    for i in range(active_channels):
        columns.append("Channel {}".format( (i+1) ))

    capture_data.columns = columns
    print("\n")
    #calculate time series statistics
    if VERBOSE >= LOGGING_INFO:
        print("Calculating Packet Delta Time Statistics")




    return capture_data



def plot_raw_channel_data(capture_data, active_channels):
    fig, ax = plt.subplots(figsize=(14,6), dpi=80)
    for i in range(active_channels):
        ax.plot(capture_data[capture_data.columns[i]], label=capture_data.columns[i], linewidth=1)
    plt.legend(loc='lower left')
    ax.set_title('Fiber Optic Acoustic Sensor Data', fontsize=16)
    plt.show()



def plot_raw_channel_fft(capture_data, active_channels, stats_dt):
    capture_data_fft = np.fft.fft(capture_data)
    T = stats_dt["Mean"]
    N = capture_data[capture_data.columns[0]].size

    fft_freq = np.linspace(0.0, 1.0/(2.0*T), N//2)
    fig, ax = plt.subplots(figsize=(14,6), dpi=80)
    for i in range(active_channels):
        ax.plot( fft_freq, 2.0/N*np.abs(capture_data_fft[:N//2,i].real), label=capture_data.columns[i], linewidth=1)
    plt.legend(loc='lower left')
    ax.set_title('Fiber Optic Acoustic Sensor Data FFT', fontsize=16)
    plt.show()



def normalize(capture_data):
    if VERBOSE >= LOGGING_INFO:
        print("Normalizing Data")
    scaler = MinMaxScaler()
    scaler_data = scaler.fit_transform(capture_data)
    return scaler, scaler_data

def export_scaled_data(scaler, output_dir):
    scaler_filename = output_dir + "_scalar_data"
    #save formatted data for later
    if VERBOSE >= LOGGING_INFO:
        print("Outputting data to {}".format(scaler_filename))
    joblib.dump(scaler, scaler_filename)

def import_scaled_data(filepath):
    scaler= joblib.load(filepath)
    return scaler


if __name__ == "__main__":
    #Get Command Line Inputs
    parser = optargs()
    options, args = parser.parse_args()

    VERBOSE = options.log_level
    PLOT = options.plot
    OUTPUT_DIR = options.output_dir

    if VERBOSE >= LOGGING_DEBUG :
        print(args)
        print(options)
    
    
    if options.command == "parse" or options.command == "parse-then-train" or options.command == "all":
        stats_dt = "none"
        capture_data = "none"

        if options.input_file == "none":
            parser.print_help()
            exit(1)
        elif path.exists(options.input_file) == False :
            print("Packet capture file not found: ", options.input_file)
            exit(1)
        elif options.file_type == "pcap":
            stats_dt, packets = parse_pcap(options.input_file, options.display_filter, NUM_ACTIVE_CHANNELS, 0, options.num_packets)
            capture_data = parse_packets(packets, options.num_packets, 100, NUM_ACTIVE_CHANNELS)
            
        elif options.file_type == "csv":
            pass
        elif options.file_type == "none":
            print("File type must be specified with -t option.\n.csv or .pcap currently supported.")
            exit(1)
        else :
            print("File type not recognized.")
            exit(1)

        if(options.plot):
            plot_raw_channel_data(capture_data, NUM_ACTIVE_CHANNELS)

        scaler, scaler_data = normalize(capture_data)

        joblib.dump(scaler, options.output_dir + "_scaler")
        joblib.dump(scaler_data, options.output_dir + "_scaler_data")
        joblib.dump(capture_data, options.output_dir + "_raw_dataFrame")











    if options.command == "train" or options.command == "parse-then-train" or options.command == "all":
        set_seed()
        
        scaler_data = joblib.load(options.input_dir + "_scaler_data")
        scaler= joblib.load(options.input_dir + "_scaler")
        raw_dataFrame = joblib.load(options.input_dir + "_raw_dataFrame")

        train_data = scaler_data.reshape(scaler_data.shape[0],1,scaler_data.shape[1])
        if VERBOSE >= LOGGING_DEBUG :
            print("Training Data Shape:", train_data.shape)

        if VERBOSE >= LOGGING_INFO :
            print("Creating Autoencoder Model")

        if options.model == "none" :
            model = autoencoder_model(train_data)
            model.compile(optimizer='adam', loss='mae')
            model.summary()

        elif path.exists(options.model) == False :
            print("Could not find model at specified path:", options.model)
            exit(1)

        else:
            #import model....
            pass

                #fit data
        nb_epochs = 100
        batch_size = 10
        history = model.fit(train_data, train_data, epochs=nb_epochs, batch_size=batch_size, validation_split=0.05).history

        model.save(options.output_dir + "_model.h5")








    if options.command == "predict" or options.command == "all":
        set_seed()

        if options.model == "none" :
            parser.print_help()
            exit(1)

        elif path.exists(options.model) == False :
            print("Could not find model at specified path:", options.model)
            exit(1)

        elif path.exists(options.input_dir) == False:
            print("Could not find files at specified path:", options.input_dir)
            exit(1)
        else:
            scaler_data = joblib.load(options.input_dir + "_scaler_data")
            scaler= joblib.load(options.input_dir + "_scaler")
            raw_dataFrame = joblib.load(options.input_dir + "_raw_dataFrame")

            train_data = scaler_data.reshape(scaler_data.shape[0],1,scaler_data.shape[1])
            model = load_model(options.model)

            #plot the loss distribution of the training set
            X_pred = model.predict(train_data)
            X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])
            X_pred = pd.DataFrame(X_pred, columns=raw_dataFrame.columns)
            X_pred.index = raw_dataFrame.index

            scored = pd.DataFrame(index=raw_dataFrame.index)
            train_data_shaped = train_data.reshape(train_data.shape[0], train_data.shape[2])
            scored['Loss_mae'] = np.mean(np.abs(X_pred-train_data_shaped), axis = 1)
            scored['Threshold'] = options.threshold
            scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']

            if options.plot :
                scored.plot(logy=True, figsize=(16,9), ylim=[1e-2, 1e2], color=['blue', 'red'])
                plt.show()


    if options.command == "none":
        parser.print_help()
        exit(1)


print("Complete!")
exit(0)
